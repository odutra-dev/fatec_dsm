{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sHsHpZ3PKjJ_",
        "q9wXbC4-MHm3",
        "qLY7O1S7NWLx"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lista de Exercícios — Introdução ao spaCy"
      ],
      "metadata": {
        "id": "HljS1yilKfD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Integrantes\n",
        "Artur Revollo; Bruno Santos; Gabriel Dutra; Luccas Lohan"
      ],
      "metadata": {
        "id": "sHsHpZ3PKjJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objetivo: praticar tokenização, POS tagging, lematização, NER, stopwords, dependências, similaridade e ajustes de vocabulário com base no notebook 'Introdução ao spaCy'. Use o modelo 'pt_core_news_sm' (spaCy v3) a menos que o notebook imponha outro.\n",
        "Dicas rápidas:\n",
        "\n",
        "- Instale: python -m spacy download pt_core_news_sm\n",
        "- Carregue: nlp = spacy.load('pt_core_news_lg')"
      ],
      "metadata": {
        "id": "nbNiZ5HwK6N0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggftuJYMJ90j",
        "outputId": "4aa35cb0-178a-4efe-dcad-141be19fb51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: spacy\n",
            "Version: 3.8.7\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel\n",
            "Required-by: fastai\n"
          ]
        }
      ],
      "source": [
        "!pip show spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCgVKTWgLcfF",
        "outputId": "bdea35f0-d1cd-4cf2-f931-9586c555b55b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: spacy 3.8.7\n",
            "Uninstalling spacy-3.8.7:\n",
            "  Would remove:\n",
            "    /usr/local/bin/spacy\n",
            "    /usr/local/lib/python3.12/dist-packages/spacy-3.8.7.dist-info/*\n",
            "    /usr/local/lib/python3.12/dist-packages/spacy/*\n",
            "Proceed (Y/n)? Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/commands/uninstall.py\", line 106, in run\n",
            "    uninstall_pathset = req.uninstall(\n",
            "                        ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/req_install.py\", line 722, in uninstall\n",
            "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/req_uninstall.py\", line 364, in remove\n",
            "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/req/req_uninstall.py\", line 404, in _allowed_to_proceed\n",
            "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/utils/misc.py\", line 235, in ask\n",
            "    response = input(message)\n",
            "               ^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/pip/_internal/cli/base_command.py\", line 215, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1576, in critical\n",
            "    def critical(self, msg, *args, **kwargs):\n",
            "\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy --upgrade\n",
        "#!pip install spacy==2.2.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxTmoGirLmB6",
        "outputId": "17687a71-ea1f-432e-b9e7-79e8f746374d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "m-rf0ycHLpoQ",
        "outputId": "2bf42530-2119-4046-d2d9-6252aea3ed8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pt_core_news_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlrLk_fhMU3E",
        "outputId": "70d4e75f-36cb-4cf9-948f-6a3e0d6d2667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.8.0/pt_core_news_lg-3.8.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregar nlp"
      ],
      "metadata": {
        "id": "q9wXbC4-MHm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('pt_core_news_lg')\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6qnYgR-MF9S",
        "outputId": "951935e4-8ec8-4a3c-c835-ec2216fa862e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.pt.Portuguese at 0x7ed0b38fa9c0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte A — Exercícios Simples\n",
        "1) Verifique versão, carregamento e pipes\n",
        "Objetivo: Checar ambiente e visão geral do pipeline.\n",
        "Tarefas:\n",
        "Imprima a versão do spaCy.\n",
        "Carregue o modelo em PT-BR e mostre os pipes com nlp.pipe_names.\n",
        "Texto de teste: “Hoje estudarei Processamento de Linguagem Natural na faculdade.”\n"
      ],
      "metadata": {
        "id": "qLY7O1S7NWLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Imprima a versão do spaCy.\n",
        "import spacy\n",
        "spacy.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xHRhY6t7NcdL",
        "outputId": "9903f94c-282d-4722-bd6a-c489751ecb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregue o modelo em PT-BR e mostre os pipes com nlp.pipe_names.\n",
        "nlp = spacy.load('pt_core_news_lg')\n",
        "nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4fQmaUbOIXt",
        "outputId": "7677e305-3903-4948-cc29-7cba501b2ad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.pt.Portuguese at 0x7ed06f7a2150>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wi500VJOpSI",
        "outputId": "a8d1b304-1dbd-4d2d-fd00-b039387695f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'morphologizer', 'parser', 'lemmatizer', 'attribute_ruler', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = nlp('Hoje estudarei Processamento de Linguagem Natural na faculdade.')"
      ],
      "metadata": {
        "id": "0vtFC2wpOz8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Xuh7oaWPE7T",
        "outputId": "14b792f0-f11e-4dc9-e9a0-088b5f1d87be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hoje estudarei Processamento de Linguagem Natural na faculdade.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2) Tokenização e POS <br>\n",
        "\n",
        "Objetivo: Observar como o spaCy separa tokens e atribui classes gramaticais.\n",
        "Tarefas:\n",
        "Tokenize e liste (token.text, token.pos_).\n",
        "Texto: “O professor apresentou exemplos claros na aula de PLN.”\n"
      ],
      "metadata": {
        "id": "RTYJ5SFVPNvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto2 = nlp('O professor apresentou exemplos claros na aula de PLN.')"
      ],
      "metadata": {
        "id": "VwC-E5FdP9qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in texto2:\n",
        "  print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqZyboabPVdp",
        "outputId": "d09115a8-b6a5-43c9-ae99-285021a920a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O DET\n",
            "professor NOUN\n",
            "apresentou VERB\n",
            "exemplos NOUN\n",
            "claros ADJ\n",
            "na ADP\n",
            "aula NOUN\n",
            "de ADP\n",
            "PLN PROPN\n",
            ". PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3) Filtre nomes próprios <br>\n",
        "\n",
        "Objetivo: Praticar filtragem por categoria POS.\n",
        "Tarefas:\n",
        "Liste apenas tokens com pos_ == 'PROPN'.\n",
        "Texto: “Ana viajou com Roberto para Florianópolis durante o feriado.”\n"
      ],
      "metadata": {
        "id": "uc2L6uEmQNcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto3 = nlp('Ana viajou com Roberto para Florianópolis durante o feriado.')"
      ],
      "metadata": {
        "id": "F8n9QSiBQRvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in texto3:\n",
        "  if token.pos_ == 'PROPN':\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYxcMQJUQrXY",
        "outputId": "99e91986-be23-4932-fdf9-d42ccd6a7710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ana\n",
            "Roberto\n",
            "Florianópolis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4) Lematização\n",
        "Objetivo: Comparar formas flexionadas e lemas. <br>\n",
        "Tarefas:\n",
        "Para cada token, imprima (token.text, token.lemma_). <br>\n",
        "Texto: “compramos compraria compraram comprarão comprando”\n"
      ],
      "metadata": {
        "id": "2NP4JGEVQ843"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto4 = nlp('compramos compraria compraram comprarão comprando')"
      ],
      "metadata": {
        "id": "YIZI4TOxRGMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in texto4:\n",
        "  print(token.text, token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiGD7MufRiJY",
        "outputId": "67c957f7-ab65-4a39-f44c-9f57f1f8e262"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compramos comprar\n",
            "compraria comprar\n",
            "compraram comprar\n",
            "comprarão comprar\n",
            "comprando comprar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5) Stopwords\n",
        "Objetivo: Praticar remoção de palavras de função. <br>\n",
        "Tarefas:\n",
        "Remova tokens que são stopwords e reconstrua a frase “limpa”. <br>\n",
        "Texto: “Eu não tenho nada contra, mas às vezes é melhor esperar.”\n"
      ],
      "metadata": {
        "id": "tbTySMoGR8RB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.pt.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "2m08RSNJSjGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(STOP_WORDS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tp2E7QpYSnLf",
        "outputId": "be7d7c9a-8e3e-4ff4-9df6-d350a3d60fa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ali', 'vinte', 'inicio', 'saber', 'nossa', 'possivelmente', 'logo', 'além', 'quieta', 'desde', 'estiveste', 'mais', 'vai', 'nível', 'des', 'puderam', 'se', 'de', 'quieto', 'certeza', 'estou', 'pontos', 'vinda', 'agora', 'tendes', 'também', 'cima', 'comprida', 'nuns', 'quais', 'quê', 'treze', 'pode', 'esses', 'longe', 'usa', 'cedo', 'bem', 'do', 'temos', 'sempre', 'meio', 'naquele', 'que', 'ver', 'só', 'fora', 'contra', 'põem', 'dezassete', 'números', 'número', 'como', 'vários', 'ponto', 'cá', 'demais', 'debaixo', 'estiveram', 'essas', 'sétimo', 'estão', 'momento', 'tarde', 'ademais', 'outros', 'cinco', 'for', 'fazer', 'oito', 'diante', 'maioria', 'outras', 'vem', 'veja', 'aqui', 'geral', 'desse', 'esta', 'dos', 'quarto', 'nessa', 'final', 'três', 'fazes', 'estivestes', 'tivemos', 'pouco', 'sei', 'era', 'quer', 'onde', 'atrás', 'fará', 'você', 'poderá', 'próxima', 'estar', 'te', 'aquilo', 'vós', 'primeiro', 'ela', 'tentaram', 'posso', 'dez', 'tão', 'tiveram', 'ambas', 'qualquer', 'favor', 'apoia', 'baixo', 'deste', 'parece', 'esse', 'devem', 'naquela', 'conselho', 'próprio', 'último', 'vossa', 'porquanto', 'desta', 'seu', 'ele', 'inclusive', 'sétima', 'deve', 'valor', 'dá', 'lado', 'ir', 'foste', 'podia', 'estivemos', 'grande', 'podem', 'estará', 'estás', 'fazia', 'por', 'sete', 'primeira', 'estava', 'poder', 'aqueles', 'mês', 'ambos', 'pouca', 'tudo', 'doze', 'forma', 'nova', 'obrigada', 'bom', 'neste', 'embora', 'seis', 'minha', 'ter', 'ontem', 'exemplo', 'cuja', 'daquela', 'quinto', 'sobre', 'nosso', 'vocês', 'caminho', 'suas', 'qual', 'vens', 'novas', 'nossas', 'sim', 'aí', 'nem', 'numa', 'irá', 'teu', 'seria', 'vez', 'alguns', 'daquele', 'sua', 'foi', 'meses', 'iniciar', 'meu', 'no', 'tens', 'nossos', 'pôde', 'querem', 'porque', 'maior', 'segunda', 'minhas', 'fazemos', 'põe', 'tanta', 'quatro', 'novo', 'quero', 'assim', 'nunca', 'vos', 'sois', 'segundo', 'tente', 'entre', 'tivestes', 'com', 'quando', 'mil', 'sem', 'tais', 'mal', 'tempo', 'nenhuma', 'dizer', 'tanto', 'fim', 'aquelas', 'os', 'à', 'eu', 'maiorias', 'bastante', 'nós', 'ainda', 'falta', 'tentei', 'somos', 'certamente', 'tua', 'vossos', 'tenho', 'área', 'local', 'faço', 'foram', 'pelos', 'vêm', 'cento', 'quinze', 'tentar', 'aquela', 'depois', 'corrente', 'usar', 'quem', 'perto', 'somente', 'num', 'cada', 'quinta', 'questão', 'sexta', 'mesmo', 'então', 'acerca', 'uma', 'talvez', 'e', 'mas', 'dezoito', 'porém', 'o', 'sabe', 'muitos', 'máximo', 'dezanove', 'estive', 'sistema', 'está', 'cujo', 'fomos', 'próximo', 'me', 'uns', 'catorze', 'onze', 'grandes', 'conhecido', 'sou', 'enquanto', 'custa', 'duas', 'elas', 'estes', 'lá', 'sob', 'diz', 'terceira', 'zero', 'lhe', 'vosso', 'contudo', 'tive', 'lugar', 'deverá', 'oitavo', 'nas', 'até', 'outra', 'ser', 'ou', 'meus', 'disso', 'dessa', 'pois', 'antes', 'essa', 'ligado', 'fazem', 'obrigado', 'umas', 'vezes', 'aos', 'pegar', 'tem', 'já', 'eles', 'adeus', 'fazeis', 'pelo', 'relação', 'é', 'teus', 'toda', 'apoio', 'algumas', 'isto', 'na', 'apenas', 'todos', 'faz', 'conhecida', 'todas', 'vossas', 'tuas', 'um', 'não', 'vão', 'todo', 'boa', 'breve', 'dentro', 'teve', 'têm', 'isso', 'nos', 'tal', 'comprido', 'nada', 'dão', 'fui', 'eventual', 'muito', 'terceiro', 'pelas', 'tiveste', 'fez', 'apontar', 'da', 'possível', 'são', 'oitava', 'este', 'fostes', 'tipo', 'partir', 'esteve', 'a', 'ora', 'às', 'vindo', 'estas', 'grupo', 'em', 'pela', 'vais', 'tu', 'direita', 'nove', 'através', 'dizem', 'após', 'dar', 'as', 'algo', 'portanto', 'novos', 'quarta', 'sexto', 'nesse', 'dois', 'das', 'povo', 'és', 'seus', 'posição', 'menor', 'aquele', 'nesta', 'para', 'estado', 'coisa', 'porquê', 'ao', 'dezasseis', 'quanto', 'menos', 'parte'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto5 = nlp('Eu não tenho nada contra, mas às vezes é melhor esperar.')"
      ],
      "metadata": {
        "id": "iLF_TzysSPlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in texto5:\n",
        "  if not nlp.vocab[token.text].is_stop:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQiGs3_FSxo8",
        "outputId": "a6ae038e-cc5a-4738-db62-a3302be5c5e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",\n",
            "melhor\n",
            "esperar\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6) NER básico + visualização\n",
        "Objetivo: Reconhecer entidades nomeadas em PT-BR. <br>\n",
        "Tarefas:\n",
        "Extraia entidades (ent.text, ent.label_). <br>\n",
        "Opcional: visualize com displacy.render(doc, style='ent', jupyter=True). <br>\n",
        "Texto: “A Apple lançou produtos em Cupertino em 2024 e planeja eventos no Brasil.”\n"
      ],
      "metadata": {
        "id": "iCbPRKU2T0aj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto6 = nlp('A Apple lançou produtos em Cupertino em 2024 e planeja eventos no Brasil.')"
      ],
      "metadata": {
        "id": "T-4lRX8JUTEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for entidade in texto6.ents:\n",
        "  print(entidade.text, entidade.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhCRmCT7U_Nr",
        "outputId": "456ade44-cbeb-4774-9ec9-e491bed988a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple ORG\n",
            "Cupertino LOC\n",
            "Brasil LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(texto6, style = 'ent', jupyter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0GR_y2wuT7vB",
        "outputId": "5bc6ef77-764f-49cd-cbaa-9107b9080301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">A \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " lançou produtos em \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Cupertino\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " em 2024 e planeja eventos no \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Brasil\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7) Similaridade entre palavras\n",
        "Objetivo: Investigar proximidade semântica entre palavras. <br>\n",
        "Tarefas: <br>\n",
        "Compare pares de tokens com token1.similarity(token2). <br>\n",
        "Pares sugeridos: (“feliz”, “contente”), (“feliz”, “triste”), (“triste”, “deprimido”).\n"
      ],
      "metadata": {
        "id": "1tOAJZb4VJtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token1 = nlp(\"feliz\")\n",
        "token2 = nlp(\"contente\")\n",
        "token3 = nlp(\"triste\")\n",
        "token4 = nlp(\"deprimido\")"
      ],
      "metadata": {
        "id": "_NmdGPIOVH3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Similaridade entre {token1} e {token2}: {nlp(token1).similarity(nlp(token2))*100:.2f} %\")\n",
        "print(f\"Similaridade entre {token1} e {token3}: {nlp(token1).similarity(nlp(token3))*100:.2f} %\")\n",
        "print(f\"Similaridade entre {token3} e {token4}: {nlp(token3).similarity(nlp(token4))*100:.2f} %\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5syUje13XKTU",
        "outputId": "47719cd0-87c9-44f7-c9d1-467cec63eb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similaridade entre feliz e contente: 53.06 %\n",
            "Similaridade entre feliz e triste: 66.62 %\n",
            "Similaridade entre triste e deprimido: 65.48 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parte B — Exercícios de Complexidade Média\n",
        "\n",
        "8) Lema vs. Stem (RSLP)\n",
        "\n",
        "Objetivo: Entender diferenças entre lematização e stemming para PT-BR.\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Monte um DataFrame com colunas: palavra, lemma_spacy, stem_rslp (NLTK).\n",
        "\n",
        "Use um conjunto misto: ['organização', 'organizar', 'organizado', 'analista', 'analisar', 'análise', 'fácil', 'facilmente'].\n",
        "\n",
        "Comente 3 casos em que stem e lema divergem e o impacto na análise."
      ],
      "metadata": {
        "id": "1IMwM6rvulmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('rslp')\n",
        "from nltk.stem import RSLPStemmer\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "RNQv1Kbxu7WX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dca1400-c3ef-4675-878b-9b9c93de7898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = RSLPStemmer()"
      ],
      "metadata": {
        "id": "5R87xPNTZaI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "palavras = ['organização', 'organizar', 'organizado', 'analista', 'analisar', 'análise', 'fácil', 'facilmente']"
      ],
      "metadata": {
        "id": "G-MQ3Mf5ZlBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(' '.join(palavras))"
      ],
      "metadata": {
        "id": "pZnkf6vpZn0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dados = []\n",
        "for token in doc:\n",
        "    dados.append({\n",
        "        \"palavra\": token.text,\n",
        "        \"lemma_spacy\": token.lemma_,\n",
        "        \"stem_rslp\": stemmer.stem(token.text)\n",
        "    })\n",
        "\n",
        "# Criar DataFrame\n",
        "df = pd.DataFrame(dados)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO-m0qobZvru",
        "outputId": "eb39518f-0cb9-4b1d-ed4e-2502515de6e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       palavra  lemma_spacy stem_rslp\n",
            "0  organização  organização   organiz\n",
            "1    organizar    organizar   organiz\n",
            "2   organizado    organizar   organiz\n",
            "3     analista     analista      anal\n",
            "4     analisar     analisar    analis\n",
            "5      análise      análise    anális\n",
            "6        fácil        fácil     fácil\n",
            "7   facilmente   facilmente     facil\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9) Dependências: origem e destino\n",
        "\n",
        "Objetivo: Usar relações de dependência para extrair slots (origem/destino).\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Identifique origem e destino sem usar índices fixos (use heads, children e preposições).\n",
        "\n",
        "Texto: “Reserve passagens saindo de Congonhas e chegando em Recife amanhã cedo.”"
      ],
      "metadata": {
        "id": "cQBV5q8vaw_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Reserve passagens saindo de Congonhas e chegando em Recife amanhã cedo.\""
      ],
      "metadata": {
        "id": "3cNEKHfjbJFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(texto)"
      ],
      "metadata": {
        "id": "ougL5_RkbWNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "origem = None\n",
        "destino = None"
      ],
      "metadata": {
        "id": "hCCzicMXco8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    if token.lemma_ == \"sair\":\n",
        "        # buscar em toda subárvore\n",
        "        for t in token.subtree:\n",
        "            # preposições marcando caso (case) ou oblíquos (obl)\n",
        "            if t.dep_ in (\"case\", \"obl\") and t.text.lower() == \"de\":\n",
        "                # o head da preposição pode ser o local\n",
        "                head = t.head\n",
        "                if head.pos_ in (\"PROPN\", \"NOUN\"):\n",
        "                    origem = head.text\n",
        "\n",
        "    elif token.lemma_ == \"chegar\":\n",
        "        for t in token.subtree:\n",
        "            if t.dep_ in (\"case\", \"obl\") and t.text.lower() == \"em\":\n",
        "                head = t.head\n",
        "                if head.pos_ in (\"PROPN\", \"NOUN\"):\n",
        "                    destino = head.text\n"
      ],
      "metadata": {
        "id": "UaILd9ACbb56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Origem detectada: {origem}\")\n",
        "print(f\"Destino detectado: {destino}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JnUmUA6bi_a",
        "outputId": "8c715c71-b6b3-4bdd-e227-8ec16e1e53f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Origem detectada: Congonhas\n",
            "Destino detectado: Recife\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10) Mapear objeto → local via dependências\n",
        "\n",
        "Objetivo: Praticar navegação em dependências para relacionar substantivos.\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Associe o objeto à localização com base em suas relações.\n",
        "\n",
        "Texto: “Precisamos de uma mesa para o restaurante e de um quarto para o hotel.”\n",
        "\n",
        "Imprima pares: mesa→restaurante, quarto→hotel."
      ],
      "metadata": {
        "id": "SoPbIEhTdT1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Precisamos de uma mesa para o restaurante e de um quarto para o hotel.\"\n",
        "doc = nlp(texto)"
      ],
      "metadata": {
        "id": "Kednn1Z4denn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pares = []"
      ],
      "metadata": {
        "id": "cMIkRGRZZmns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    # Verifica se o token é uma preposição \"para\"\n",
        "    if token.text.lower() == \"para\" and token.dep_ == \"case\":\n",
        "        # O head dessa preposição geralmente é o local (ex: \"restaurante\")\n",
        "        local = token.head\n",
        "        if local.pos_ in (\"NOUN\", \"PROPN\"):\n",
        "            # Procurar o objeto correspondente: o substantivo que rege o local\n",
        "            # Subimos para o objeto via o head do local\n",
        "            objeto = local.head\n",
        "            if objeto.pos_ in (\"NOUN\", \"PROPN\"):\n",
        "                pares.append((objeto.text, local.text))\n"
      ],
      "metadata": {
        "id": "dA4kk7fPdg0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pares)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iHP1574dn6B",
        "outputId": "b91c5144-b0a5-4b49-e758-0a0ffee0ee10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('mesa', 'restaurante'), ('quarto', 'hotel')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 11) Ações vinculadas a lugares\n",
        "\n",
        "Objetivo: Generalizar padrão de ações ligadas a locais.\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Extraia pares lugar → verbo associado (ancestors/head).\n",
        "\n",
        "Texto: “Quais museus podemos visitar em Lisboa e o que podemos conhecer em Sintra?”"
      ],
      "metadata": {
        "id": "gXEtuqsteSDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = 'Quais museus podemos visitar em Lisboa e o que podemos conhecer em Sintra?'\n",
        "doc = nlp(texto)"
      ],
      "metadata": {
        "id": "M7Hw1F2RgJMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"LOC\":\n",
        "        # pegar o token cabeça do lugar\n",
        "        head = ent.root.head\n",
        "        if head.pos_ == \"VERB\":\n",
        "            print(f\"{ent.text} → {head.lemma_}\")\n"
      ],
      "metadata": {
        "id": "8qoqCA8agcIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75093050-fdbf-444e-a087-1c9c007557db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lisboa → visitar\n",
            "Sintra → conhecer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 12) Similaridade de sentenças com ranking\n",
        "\n",
        "Objetivo: Comparar semântica de sentenças curtas.\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Dado o conjunto A, B, C, calcule similaridades A↔B e A↔C e explique resultados.\n",
        "\n",
        "A: “Quando será inaugurado o novo campus?”\n",
        "\n",
        "B: “O novo campus será inaugurado no próximo semestre.”\n",
        "\n",
        "C: “Quantas vagas restaram para o curso noturno?”\n",
        "\n"
      ],
      "metadata": {
        "id": "nIFAI5VKVjzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textoA = nlp('Quando será inaugurado o novo campus?')\n",
        "textoB = nlp('O novo campus será inagurado no próximo semestre.')\n",
        "textoC = nlp('Quantas vagas restaram para o curso noturno?')"
      ],
      "metadata": {
        "id": "aMAEAmCyV5FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textoA.similarity(textoB)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYJEbJOfXIb3",
        "outputId": "dac601b5-f5b2-4377-f1a6-8f2756a6ab07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7344844341278076"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textoA.similarity(textoC)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gb5YGQeGXR1V",
        "outputId": "6f91bcfe-614c-4dee-ffcc-8b4415576f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6720597743988037"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A similaridade entre A e B foi de 0.73, mostrando alta proximidade, já que ambas tratam da inauguração do novo campus (pergunta e resposta). Já entre A e C o valor foi de 0.67, que saiu relativamente alto, mas na prática os temas são diferentes (inauguração do campus x vagas no curso noturno). Isso acontece porque o modelo considera palavras em comum e proximidade geral, mas nem sempre consegue diferenciar totalmente o contexto."
      ],
      "metadata": {
        "id": "4HWqAsYeYmRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 13) Stopwords personalizadas (negadores e domínio)\n",
        "\n",
        "Objetivo: Ajustar vocabulário às necessidades da tarefa.\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Adicione 'curso' e 'prova' às stopwords e remova 'não' e 'nunca'.\n",
        "\n",
        "Reaplique em: “Eu não vou faltar à prova do curso, nunca!”\n",
        "\n",
        "Discuta como preservar negação influencia análises de sentimento."
      ],
      "metadata": {
        "id": "uUcIDSX4YwUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.lang.pt.stop_words import STOP_WORDS"
      ],
      "metadata": {
        "id": "YjLmt8rVbkTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar 'curso' e 'prova' às stopwords\n",
        "STOP_WORDS |= {\"curso\", \"prova\"}\n",
        "\n",
        "# Remover 'não' e 'nunca' das stopwords\n",
        "STOP_WORDS -= {\"não\", \"nunca\"}\n",
        "\n",
        "# Texto\n",
        "texto = \"Eu não vou faltar à prova do curso, nunca!\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Tokens sem stopwords\n",
        "tokens_filtrados = [token.text for token in doc if token.text.lower() not in STOP_WORDS]\n",
        "print(tokens_filtrados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2PJKxRqZ1DK",
        "outputId": "6b81f3e9-9207-4940-aa56-558c95bc5ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['não', 'vou', 'faltar', ',', 'nunca', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Depois de ajustar as stopwords, o resultado ficou ['não', 'vou', 'faltar', ',', 'nunca', '!']. As palavras curso e prova foram removidas porque agora são stopwords, enquanto não e nunca foram preservadas para manter a negação. Isso é importante em análise de sentimentos, pois a negação pode mudar totalmente o sentido de uma frase."
      ],
      "metadata": {
        "id": "f_9mPYcvb-ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 14) Tokenização de casos especiais + limpeza com .is_right_punct\n",
        "\n",
        "Objetivo: Analisar splits e aplicar filtro com .is_right_punct no pré-processamento.\n",
        "\n",
        "Tarefas:\n",
        "\n",
        "Tokenize e liste tokens obtidos.\n",
        "\n",
        "Texto: “Promoção: e-commerce com frete grátis por R$ 1.299,90 — aproveite já!”\n",
        "\n",
        "Remova tokens de pontuação à direita usando token.is_right_punct e reconstrua o texto limpo.\n",
        "\n",
        "Comente se a tokenização ajuda ou atrapalha a extração de preço."
      ],
      "metadata": {
        "id": "n3y2TAXZcMk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = 'Promoção: e-commerce com frete grátis por R$ 1.299,90 — aproveite já!'\n",
        "doc = nlp(texto)"
      ],
      "metadata": {
        "id": "CFo_VpeWdJIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaqskqJqdrHV",
        "outputId": "f2789c6b-fcc1-42a1-c65c-9219b238a657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoção\n",
            ":\n",
            "e-commerce\n",
            "com\n",
            "frete\n",
            "grátis\n",
            "por\n",
            "R$\n",
            "1.299,90\n",
            "—\n",
            "aproveite\n",
            "já\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.text for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2R3cbS8e8KI",
        "outputId": "2735c902-1de0-4412-bad5-53a7775fffed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Promoção', ':', 'e-commerce', 'com', 'frete', 'grátis', 'por', 'R$', '1.299,90', '—', 'aproveite', 'já', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_filtrados = [token.text for token in doc if not token.is_right_punct]\n",
        "print(tokens_filtrados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbAddyxWeD2i",
        "outputId": "03e199a0-6158-401e-e46f-480eab663773"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Promoção', ':', 'e-commerce', 'com', 'frete', 'grátis', 'por', 'R$', '1.299,90', '—', 'aproveite', 'já', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto_limpo = \" \".join(tokens_filtrados)\n",
        "print(texto_limpo)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uptBUC3ofYBU",
        "outputId": "5dfc6818-7451-4c8c-e953-948bb3a7545f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Promoção : e-commerce com frete grátis por R$ 1.299,90 — aproveite já !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#A tokenização ajuda porque mantém R$ e 1.299,90 separados, facilitando a extração do valor. Se o número fosse quebrado em múltiplos tokens, seria mais difícil extrair o preço."
      ],
      "metadata": {
        "id": "It1JE_U-i1cY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}